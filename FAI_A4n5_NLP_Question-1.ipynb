{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iDD1Ik_LiN4"
   },
   "source": [
    "# Text Generation\n",
    "\n",
    "So.... we have seen AI is capable of generating text in last week lecture. The model used is very sophisticated (GPT3 model by openai) and it's not an easy task to replicate the feat. Instead, we are exploring to build a simple language model in this homework, namely, the **N-Gram model**. First of all, some key words to understand:\n",
    "\n",
    "\n",
    "1.   Language Model -- A model that assign probabilities to sequences of words.\n",
    "2.   N-gram -- A sequence of *n* words. For example, 1-gram (unigram), 2-gram (bigram), 3-gram (trigram), ... Feel free to quickly Google to learn more about N-gram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0FGFreAWhYE"
   },
   "source": [
    "## make_ngram() and pad()\n",
    "\n",
    "First, let's implement a function to generate ngrams. Take a look on how you could implement it using a sliding window technique:  https://www.researchgate.net/publication/354427958/figure/fig1/AS:1065576361369600@1631064610082/Extraction-of-n-grams-with-a-sliding-window-approach-n-3.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WoXOu74pNhG8"
   },
   "outputs": [],
   "source": [
    "def make_ngrams(list_of_words, n):\n",
    "    '''\n",
    "    For example, list_of_words = ['I', 'am', 'happy']\n",
    "    make_ngrams(list_of_words, 1) returns [['I'], ['am'], ['happy']]\n",
    "    make_ngrams(list_of_words, 2) returns [['I', 'am'], ['am', 'happy']]\n",
    "    make_ngrams(list_of_words, 3) returns [['I', 'am', 'happy']]\n",
    "    '''\n",
    "    tokens = [token for token in list_of_words if token != \"\"]\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram).split() for ngram in ngrams]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pv9FWLVUOoBb"
   },
   "outputs": [],
   "source": [
    "# Check your work\n",
    "input = \"I am happy\".split(\" \")\n",
    "assert make_ngrams(input, 1) == [['I'], ['am'], ['happy']]\n",
    "assert make_ngrams(input, 2) == [['I', 'am'], ['am', 'happy']]\n",
    "assert make_ngrams(input, 3) == [['I', 'am', 'happy']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDYB61GnQAcb"
   },
   "source": [
    "In practice, we usually \"pad\" the input sentences with starting and ending tokens (think of it as special \"words\"), according to the *n* used in n-grams.  For example, we would pad the sentence \"I am happy\" with \\<s\\> and \\<\\\\s\\> tokens so the input to make_ngrams() looks like ['\\<s\\>', 'I', 'am', 'happy', '\\<\\\\s\\>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "a5O8lJ4GVEQI"
   },
   "outputs": [],
   "source": [
    "def pad(list_of_words, n):\n",
    "    '''\n",
    "    For example, list_of_words = ['I', 'am', 'happy']\n",
    "    pad(list_of_words, n =1) does nothing and return ['I', 'am', 'happy']\n",
    "    pad(list_of_words, n =2) returns ['<s>', 'I', 'am', 'happy', '<\\s>']\n",
    "    pad(list_of_words, n =3) returns ['<s>', '<s>', 'I', 'am', 'happy', '<\\s>', '<\\s>']\n",
    "    '''\n",
    "    for i in range(n-1):\n",
    "        list_of_words= ['<s>']+ list_of_words+ ['</s>']\n",
    "    return list_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Lnu75oJ2VoXO"
   },
   "outputs": [],
   "source": [
    "# Check your work\n",
    "input = \"I am happy\".split(\" \")\n",
    "assert pad(input, 1) == ['I', 'am', 'happy']\n",
    "assert pad(input, 2) == ['<s>', 'I', 'am', 'happy', '</s>']\n",
    "assert pad(input, 3) == ['<s>', '<s>', 'I', 'am', 'happy', '</s>', '</s>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3GKgyReV1pX"
   },
   "source": [
    "Now, try pad() a sentence first, and then make_ngrams() on the padded sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4DFA9z9eWCgv"
   },
   "outputs": [],
   "source": [
    "NGRAM = 3\n",
    "input = \"I am happy\".split(\" \")\n",
    "padded_input = pad(input, NGRAM)\n",
    "expected = [\n",
    "    ['<s>', '<s>', 'I'],\n",
    "    ['<s>', 'I', 'am'],\n",
    "    ['I', 'am', 'happy'],\n",
    "    ['am', 'happy', '</s>'],\n",
    "    ['happy', '</s>', '</s>']\n",
    "]\n",
    "assert make_ngrams(padded_input, NGRAM) == expected\n",
    "\n",
    "input = \"I am sad\".split(\" \")\n",
    "padded_input = pad(input, NGRAM)\n",
    "expected = [\n",
    "    ['<s>', '<s>', 'I'],\n",
    "    ['<s>', 'I', 'am'],\n",
    "    ['I', 'am', 'sad'],\n",
    "    ['am', 'sad', '</s>'],\n",
    "    ['sad', '</s>', '</s>']\n",
    "]\n",
    "assert make_ngrams(padded_input, NGRAM) == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0-dKsVxYjgq"
   },
   "source": [
    "## Big Picture\n",
    "\n",
    "Now that we understand padding and N-grams, how can we use it to \"Generate\" text? Here is what we are going to do:\n",
    "\n",
    "1. Grab some text data, pad it, and generate n-grams\n",
    "2. Records the n-grams counts in a Python dictionary.\n",
    "3. Use that dictionary to sample and generate text.\n",
    "\n",
    "For example, let say our data simply consists of two sentences:  \n",
    "```\n",
    "[  \n",
    "  [\"I\", \"am\", \"happy\"],  \n",
    "  [\"I\", \"am\", \"sad\"]  \n",
    "]  \n",
    "```\n",
    "and we use `NGRAM = 3`. Then, we can records the counts of n-grams like:\n",
    "\n",
    "| {n-1}gram       | word   | Count |\n",
    "|-----------------|--------|-------|\n",
    "| \\<s\\> ,  \\<s\\>  | I      | 2     |\n",
    "| \\<s\\> ,  I      | am     | 2     |\n",
    "| I ,  am         | happy  | 1     |\n",
    "| I ,  am         | sad    | 1     |\n",
    "| am ,  happy     | \\</s\\> | 1     |\n",
    "| am ,  sad       | \\</s\\> | 1     |\n",
    "| happy ,  \\</s\\> | \\</s\\> | 1     |\n",
    "| sad ,  \\</s\\>   | \\</s\\> | 1     |\n",
    "\n",
    "We essentially take every n-grams, split them into `(n-1)-grams` and a `word`, and store them into a dictionary. If the same n-grams appear again, we increment the count of seeing such a combination. The following combinations has `Count` of 2 because they appear in both sentences.\n",
    "\n",
    "| {n-1}gram       | word   | Count |\n",
    "|-----------------|--------|-------|\n",
    "| \\<s\\> ,  \\<s\\>  | I      | 2     |\n",
    "| \\<s\\> ,  I      | am     | 2     |\n",
    "\n",
    "Later on, when we want to generate sentence, we can use the `Count` to guide us.   \n",
    "For example, given `[I, am]` as the `(n-1)-grams`, we want to \"generate\" the next word. The following shows that the words `happy` and `sad` are equally probable, so the probability of generating `happy` is 0.5 and the same goes to `sad`\n",
    "\n",
    "| {n-1}gram       | word   | Count |\n",
    "|-----------------|--------|-------|\n",
    "| I ,  am         | happy  | 1     |\n",
    "| I ,  am         | sad    | 1     |\n",
    "\n",
    "Assuming that our data instead is   \n",
    "```\n",
    "[  \n",
    "  [\"I\", \"am\", \"happy\"],  \n",
    "  [\"I\", \"am\", \"happy\"],  \n",
    "  [\"I\", \"am\", \"sad\"]  \n",
    "]  \n",
    "```\n",
    "The dictionary would look like:   \n",
    "\n",
    "| {n-1}gram       | word   | Count |\n",
    "|-----------------|--------|-------|\n",
    "| I ,  am         | happy  | 2     |\n",
    "| I ,  am         | sad    | 1     |\n",
    "\n",
    "so it's more likely to generate `happy` as the next word. We will revisit this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RU29Na7eWov9"
   },
   "source": [
    "## Acquiring data\n",
    "\n",
    "Read the Shakespeare Play data from [here](https://raw.githubusercontent.com/TheanLim/cs6120/main/shakespeare_plays.txt). The text is \"cleaned\" such that all words are in lower case, and all punctuations are stripped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0A1maaBBPifY"
   },
   "outputs": [],
   "source": [
    "# Read and store the text into a variable named `data`\n",
    "# `data` should be a list of list, where each inner list represents a line of the text\n",
    "import requests\n",
    "\n",
    "link = \"https://raw.githubusercontent.com/TheanLim/cs6120/main/shakespeare_plays.txt\"\n",
    "raw_text= requests.get(link).text\n",
    "data = raw_text.split( '\\n' )[:-1]\n",
    "\n",
    "assert len(data) == 29959"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mvBkp5SGkWQk"
   },
   "outputs": [],
   "source": [
    "# Split each sentence by white spaces\n",
    "data = [sentence.split(\" \") for sentence in data]\n",
    "assert len(data) == 29959"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2sj7JF6YG-E"
   },
   "source": [
    "## Pad the data and create n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "27_pzWdVXfBr"
   },
   "outputs": [],
   "source": [
    "NGRAM = 5\n",
    "# Create Ngrams for each sentence in data\n",
    "## TODO\n",
    "data_ngrams= [make_ngrams(pad(sentence, NGRAM), NGRAM) for sentence in data]\n",
    "assert len(data_ngrams[0][0]) == NGRAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXEt21ErlIWA"
   },
   "source": [
    "## Records the n-grams counts in a Python dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8yCZLlHlbtU"
   },
   "source": [
    "While I previously represented the dictionary as a table:\n",
    "\n",
    "| {n-1}gram       | word   | Count |\n",
    "|-----------------|--------|-------|\n",
    "| \\<s\\> ,  \\<s\\>  | I      | 2     |\n",
    "| \\<s\\> ,  I      | am     | 2     |\n",
    "| I ,  am         | happy  | 1     |\n",
    "| I ,  am         | sad    | 1     |\n",
    "| am ,  happy     | \\</s\\> | 1     |\n",
    "| am ,  sad       | \\</s\\> | 1     |\n",
    "| happy ,  \\</s\\> | \\</s\\> | 1     |\n",
    "| sad ,  \\</s\\>   | \\</s\\> | 1     |\n",
    "\n",
    "it's more natural to represent it using Python Nested Dictionary\n",
    "```\n",
    "ngram_dict = \n",
    "{\n",
    "  '<s>,<s>': {'I': 2},\n",
    "  '<s>,I': {'am': 2},\n",
    "  'I,am': {'happy': 1, 'sad': 1},\n",
    "  'am,happy': {'</s>': 1},\n",
    "  'am,sad': {'</s>': 1},\n",
    "  'happy,</s>': {'</s>': 1},\n",
    "  'sad,</s>': {'</s>': 1}\n",
    " }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KwinuVaFoEK2"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "ngram_dict = defaultdict(lambda: defaultdict(int))\n",
    "NGRAM = 5\n",
    "\n",
    "for list_of_words in data_ngrams:\n",
    "    for ngram in list_of_words:\n",
    "        prefix= \",\".join(ngram[:NGRAM- 1])\n",
    "        subfix= ngram[-1]\n",
    "        ngram_dict[prefix][subfix]+= 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ki8qd-wnqu8l"
   },
   "source": [
    "## Generate Sentences\n",
    "\n",
    "Here is the exciting part - Text Generation. Our strategy here is to generate one word at a time, until we see the ending token `</s>`.\n",
    "\n",
    "* We use (n-1)-grams to get a list of possible `next_words`, as well as its associated `Counts`. \n",
    "* We then sample and pick one word from the list of `next_words`, using those `Counts` as the probability (aka weights). Take a look at `random.choices(population, weights)`\n",
    "* Since we padded sentences to start with `<s>`, we should also use that as the start in our sentences generation. In a `NGRAM = 3` example, we want to start with two `<s>`.\n",
    "\n",
    "\n",
    "\n",
    "**Note:**  \n",
    "The `ngram_dict` will always have a key made up of `NGRAM-1` `<s>`. If `NGRAM = 3`, then `ngram_dict[<s>,<s>]` will always return you something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "iTl4TmBBpOVb"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_word(prompt, ngram_dict):\n",
    "    '''\n",
    "    Given a prompt, sample and return a next word from ngram_dict\n",
    "    The prompt should be made up of NGRAM-1 words\n",
    "    and should be a legitimate key of ngram_dict\n",
    "    '''\n",
    "    population = ngram_dict[prompt]\n",
    "    keys= list(population.keys())\n",
    "    weights= []\n",
    "    for key in keys:\n",
    "        weights.append(population[key])\n",
    "    if keys and len(keys)> 0:\n",
    "        next_word = random.choices(keys, weights)\n",
    "        return next_word\n",
    "    return [\"</s>\"]\n",
    "\n",
    "def generate_sentence(n, ngram_dict, NGRAM):\n",
    "    '''\n",
    "    Generate a sentence with a maximum of `n` words.\n",
    "\n",
    "    Note: \n",
    "    - Do not output <s> or </s>. The output sentence should look like a normal sentence\n",
    "    - You should call generate_word() from this function, and stop when the return word is </s>\n",
    "    '''\n",
    "    res = []\n",
    "    cur = [\"<s>\"]* (NGRAM- 1)\n",
    "#     prompt= \",\".join(cur)\n",
    "#     next_word = generate_word( prompt, ngram_dict)\n",
    "#     cur= cur[1:]+ next_word\n",
    "#     print (cur)\n",
    "    while n> 0:\n",
    "        prompt= \",\".join(cur)\n",
    "        next_word = generate_word( prompt, ngram_dict)\n",
    "        if next_word == \"</s>\":\n",
    "            break\n",
    "        cur= cur[1:]+ next_word\n",
    "        res+= next_word\n",
    "        \n",
    "        n-= 1\n",
    "#     return \" \".join(res)\n",
    "    return \" \".join([word for word in res if word not in [\"<s>\", \"</s>\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "T4cuGp8isH8Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find his way without his eyes for out o doors he went without their help and to the last bended\n",
      "\n",
      "macbeth give him tending he brings great news exit messenger the raven himself is hoarse that croaks the fatal entrance\n",
      "\n",
      "give her good watch i pray you exit horatio o this is the poison of deep grief it springs all\n",
      "\n",
      "very distant time stood as it were in\n",
      "\n",
      "i cannot make true wars i ll frame convenient peace now good aufidius were you in my stead would you\n",
      "\n",
      "you sir desire my man s abode where i did leave him with his forces and do expect him here\n",
      "\n",
      "greater he shall not be if he serve god we ll serve him too and be his fellow so revolt\n",
      "\n",
      "hand and with a thought seven of the eleven i paid prince o monstrous eleven buckram men grown out of\n",
      "\n",
      "cast your election on him sicinius say you chose him more after our commandment than as guided by your own\n",
      "\n",
      "immortal title to your crown king richard we thank you both yet one but flatters us as well appeareth by\n",
      "\n",
      "parolles a filthy officer he is in those suggestions for the young earl beware of them diana their promises enticements\n",
      "\n",
      "first a very excellent good conceited thing after a wonderful sweet air with admirable rich words to it and then\n",
      "\n",
      "pardon from himself though you and all the kings of christendom are led so grossly by this\n",
      "\n",
      "here i lay and thus i bore my point four rogues in buckram let drive at me prince what four\n",
      "\n",
      "disdaining me and throwing favours on the low posthumus slanders so her judgment that what s else rare is chok\n",
      "\n",
      "done bora we ll wait upon your lordship exeunt scene 3 pomfret castle enter sir richard ratcliff with halberds carrying\n",
      "\n",
      "dangerous third watchman ay but give me worship and quietness i like it better than dangerous honour if warwick knew\n",
      "\n",
      "worm cleopatra get thee hence farewell clown i wish you all joy of the worm sets down the basket cleopatra\n",
      "\n",
      "this thou shalt not choose but go do not deny beshrew his soul for me he started one poor heart\n",
      "\n",
      "laugh but in that maid s company but indeed she is given too much to allicholy and musing but for\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate 20 sentences with a maximum 20 words each\n",
    "for i in range(20):\n",
    "    print(generate_sentence(20, ngram_dict, NGRAM))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37mncxa0-RUb"
   },
   "source": [
    "# Reference\n",
    "\n",
    "1. https://web.stanford.edu/~jurafsky/slp3/3.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMEC0NGa-jz9"
   },
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\python310\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: joblib in c:\\python310\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\python310\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\python310\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python310\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: colorama in c:\\python310\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ip (c:\\python310\\lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip available: 22.2.2 -> 22.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\tongz\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\tongz\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: click in c:\\users\\tongz\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\tongz\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tongz\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from nltk) (4.64.1)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\tongz\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages (from click->nltk) (0.4.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 20.0.2; however, version 22.3.1 is available.\n",
      "You should consider upgrading via the 'c:\\users\\tongz\\appdata\\local\\programs\\python\\python38-32\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fDzA6cny-mgn",
    "outputId": "353e0217-96d0-47b7-8cdc-d9f48206e813"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\tongz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk, random\n",
    "from nltk.corpus import movie_reviews\n",
    "import random\n",
    "nltk.download(\"movie_reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "ScObou8dAau0"
   },
   "outputs": [],
   "source": [
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "# Each document is a tuple of (list_of_words, str_of_pos_or_neg_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EkJusmiWA39K",
    "outputId": "da2511c6-4581-4e00-edd2-73c3da901bf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'neg': 1000, 'pos': 1000})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "count_pos_neg = Counter([label for (words, label) in documents])\n",
    "print(count_pos_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeCpcU6ZBiSV"
   },
   "source": [
    "## Split data into train and test\n",
    "\n",
    "Make sure that the positive and negative labels are balanced in the train and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WjghT0nvBfDE",
    "outputId": "cbff1324-af1a-46db-fcd0-895130553179"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'neg': 855, 'pos': 845})\n",
      "Counter({'pos': 155, 'neg': 145})\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "## TODO\n",
    "train, test = train_test_split(documents, test_size=0.15, random_state= 32)\n",
    "\n",
    "print(Counter([label for (words, label) in train]))\n",
    "print(Counter([label for (words, label) in test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "TTz_5pCdCDgm"
   },
   "outputs": [],
   "source": [
    "# Split data into X and Y\n",
    "## TODO\n",
    "X_train = [ele[0] for ele in train]\n",
    "X_test = [ele[0] for ele in test]\n",
    "y_train = [ele[1] for ele in train]\n",
    "y_test = [ele[1] for ele in test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJE9So4lCKOS"
   },
   "source": [
    "## Bag of Words \n",
    "We are going to use Bag of Words as \"Features\". Take a look at this [image](https://miro.medium.com/max/1400/1*hLvya7MXjsSc3NS2SoLMEg.webp)  \n",
    "1. Create a vocabulary of words. Only keep a word if it appears more than 10 times.\n",
    "2. Use each word in the vocabulary as a \"feature\". We record whet how many times a word appear within a review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "esvSYGa8Ie1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8817\n"
     ]
    }
   ],
   "source": [
    "MIN_COUNT = 10\n",
    "## TODO  Create a vocab.  Only keep a word if it appears more than MIN_COUNT times\n",
    "cnt= defaultdict(int)\n",
    "vocab= {}\n",
    "for words in [ele[0] for ele in documents]:\n",
    "    for word in words:\n",
    "        cnt[word]+= 1\n",
    "idx= 0\n",
    "for (key, val) in cnt.items():\n",
    "    if val> MIN_COUNT:\n",
    "        vocab[key]= idx\n",
    "        idx+= 1\n",
    "print (len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "oogZIdWtEgN_"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def create_features(review, vocab):\n",
    "    '''\n",
    "    Create and return a list of features given a movie review\n",
    "\n",
    "    '''\n",
    "    features = [0] * len (vocab)\n",
    "    for word in review:\n",
    "        if word in vocab:\n",
    "            features[vocab[word]] += 1\n",
    "    assert len(features) == len (vocab)\n",
    "    return [int( f) for f in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "q5N9HcP_FGad"
   },
   "outputs": [],
   "source": [
    "# Create features for each movie review\n",
    "X_train_features = [create_features(review, vocab) for review in X_train]\n",
    "X_test_features = [create_features(review, vocab) for review in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (X_train_features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uITp5nVjFsC0"
   },
   "source": [
    "## Fit a Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "i1x3nB_TFI3k"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = LogisticRegression(random_state=64, solver='lbfgs', max_iter=1000).fit(X_train_features, y_train)\n",
    "y_pred = clf.predict(X_test_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "print (\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDsM65dlU-q2"
   },
   "source": [
    "# How does your model perform on the test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUlwgH1JGkFB"
   },
   "source": [
    "I tried several different sets of parameters and was able to get stable results\n",
    "If we want to go further, we can normalize the FEATURES."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
